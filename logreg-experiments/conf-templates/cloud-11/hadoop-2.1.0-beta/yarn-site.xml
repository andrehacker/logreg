<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>

<!-- Site specific YARN configuration properties -->

  <!--
    Other properties derived from yarn.resourcemanager.hostname:
    yarn.resourcemanager.address
    yarn.resourcemanager.resource-tracker.address
    yarn.resourcemanager.scheduler.address
    yarn.resourcemanager.admin.address
    yarn.resourcemanager.webapp.address: 8088
  -->
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>cloud-11</value>
  </property>

  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
    <default>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</default>
    <description>The class to use as the resource scheduler.</description>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce.shuffle</value>
    <description>Shuffle service that needs to be set for Map Reduce applications. (does not work without this setting)</description>
    <hadoopbook>YARN doesnâ€™t have tasktrackers to serve map outputs to reduce tasks, so for this function it relies on shuffle handlers, which are long-running auxiliary services running in
node managers. Since YARN is a general-purpose service the shuffle handlers need to
be explictly enabled in the  yarn-site.xml</hadoopbook>
  </property>

  <!-- LOG AGGREGATION -->
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>false</value>
    <default>false</default>
    <description>Whether to enable log aggregation</description>
  </property>
  <property>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>/experiments/log-aggregates</value>
    <default>/tmp/logs</default>
    <description>
      HDFS directory where the application logs are moved on application completion. Need to set appropriate permissions. Only applicable if log-aggregation is enabled.
    </description>
  </property>
  <property>
    <name>yarn.nodemanager.remote-app-log-dir-suffix</name>
    <value>logs</value>
    <default>logs</default>
    <description>
      logs The remote log dir will be created at {yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam}
      (Suffix appended to the remote log dir. Logs will be aggregated to ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} Only applicable if log-aggregation is enabled.)
    </description>
  </property>
  <!-- LOG AGGREGATION END -->


  <!-- NODE MANAGER MEMORY -->
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>29696</value>
    <default>8192</default>
    <description>Amount of physical memory, in MB, that can be allocated for containers.
      (Defines total available resources on the NodeManager to be made available to running containers)</description>
  </property>

  <property>
    <name>yarn.nodemanager.vmem-pmem-ratio</name>
    <value>2.1</value>
    <default>2.1</default>
    <description>
      Ratio between virtual memory to physical memory when setting memory limits for containers. Container allocations are expressed in terms of physical memory, and virtual memory usage is allowed to exceed this allocation by this ratio.
      (The virtual memory usage of each task may exceed its physical memory limit by this ratio. The total amount of virtual memory used by tasks on the NodeManager may exceed its physical memory usage by this ratio.)
    </description>
  </property>
  <!-- NODE MANAGER MEMORY END -->


  <!-- RESSOURCE MANAGER MEMORY -->
  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>1024</value>
    <default>1024</default>
    <tuning-note>
        Yarn can only allocate containers with a size that is the multiple of this. Makes sense to leave this at default, so that the maximum memory can be used (yarn.nodemanager.resource.memory-mb)
        mapreduce.map.memory.mb and mapreduce.map.reduce.mb can define more fine grade how much memory they want
    </tuning-note>
    <description>The minimum allocation for every container request at the RM, in MBs. Memory requests lower than this won't take effect, and the specified value will get allocated at minimum.</description>
  </property>

  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>8192</value>
    <default>8192</default>
    <description> The maximum allocation for every container request at the RM, in MBs. Memory requests higher than this won't take effect, and will get capped to this value.</description>
  </property>
  <!-- RESSOURCE MANAGER MEMORY END -->

</configuration>
